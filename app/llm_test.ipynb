{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device_type, model_id, model_basename=None):\n",
    "    \"\"\"\n",
    "    Select a model for text generation using the HuggingFace library.\n",
    "    If you are running this for the first time, it will download a model for you.\n",
    "    subsequent runs will use the model from the disk.\n",
    "\n",
    "    Args:\n",
    "        device_type (str): Type of device to use, e.g., \"cuda\" for GPU or \"cpu\" for CPU.\n",
    "        model_id (str): Identifier of the model to load from HuggingFace's model hub.\n",
    "        model_basename (str, optional): Basename of the model if using quantized models.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        HuggingFacePipeline: A pipeline object for text generation using the loaded model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported model or device type is provided.\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    if model_basename is not None:\n",
    "        if \".ggml\" in model_basename:\n",
    "            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
    "            max_ctx_size = 2048\n",
    "            kwargs = {\n",
    "                \"model_path\": model_path,\n",
    "                \"n_ctx\": max_ctx_size,\n",
    "                \"max_tokens\": max_ctx_size,\n",
    "            }\n",
    "            if device_type.lower() == \"mps\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "            if device_type.lower() == \"cuda\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "                kwargs[\"n_batch\"] = max_ctx_size\n",
    "            return LlamaCpp(**kwargs)\n",
    "\n",
    "        else:\n",
    "            # The code supports all huggingface models that ends with GPTQ and have some variation\n",
    "            # of .no-act.order or .safetensors in their HF repo.\n",
    "\n",
    "            if \".safetensors\" in model_basename:\n",
    "                # Remove the \".safetensors\" ending if present\n",
    "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "            model = AutoGPTQForCausalLM.from_quantized(\n",
    "                model_id,\n",
    "                model_basename=model_basename,\n",
    "                use_safetensors=True,\n",
    "                trust_remote_code=True,\n",
    "                device=\"cuda:0\",\n",
    "                use_triton=False,\n",
    "                quantize_config=None,\n",
    "            )\n",
    "    elif (\n",
    "        device_type.lower() == \"cuda\"\n",
    "    ):  # The code supports all huggingface models that ends with -HF or which have a .bin\n",
    "        # file in their HF repo.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
    "        )\n",
    "        model.tie_weights()\n",
    "    else:\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "    # Load configuration from the model to avoid warnings\n",
    "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "    # see here for details:\n",
    "    # https://huggingface.co/docs/transformers/\n",
    "    # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n",
    "\n",
    "    # Create a pipeline for text generation\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=2048,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "input_string =  \"\"\" Sure, here is a horror story in the format of a list:[\"The old mansion loomed before us\", \"Crickets chirped in the darkness\", \"Rachel just returned home\", \"Her eyes scanned the living room\", \"But something felt off\", \"She noticed the flickering candles\", \"And the eerie silence\"]\n",
    "I hope this helps! Let me know if you have any questions.\"\n",
    "\"\"\"\n",
    "print (type(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The old mansion loomed before us',\n",
       " 'Crickets chirped in the darkness',\n",
       " 'Rachel just returned home',\n",
       " 'Her eyes scanned the living room',\n",
       " 'But something felt off',\n",
       " 'She noticed the flickering candles',\n",
       " 'And the eerie silence']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "input = '''\n",
    "Sure! Here is a horror story in the format of a python list:\n",
    "\n",
    "[\"The old mansion loomed before us\", \"Creepy noises echoed through the halls\", \"We had to investigate\", \"As we ventured deeper into the house\", \"Ominous shadows danced on the walls\", \"A faint scent of decay wafted through the air\", \"Rachel stumbled upon a hidden room\", \"Inside, we found unspeakable horrors\", \"Our screams were drowned out by the howling wind\", \"We fled in terror, but knew we'd never escape\"]\n",
    "'''\n",
    "def string_to_list(input):\n",
    "\n",
    "    # Find the list using regular expression\n",
    "    match = re.search(r'\\[.*?\\]', input_string, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        extracted_list_str = match.group(0)\n",
    "    \n",
    "        # Convert the extracted string to a list using ast.literal_eval\n",
    "        extracted_list = ast.literal_eval(extracted_list_str)\n",
    "    \n",
    "        return extracted_list\n",
    "    else:\n",
    "        print(\"No list found in the input string.\")\n",
    "\n",
    "string_to_list(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here is a horror story in the format of a list:\n",
      "\n",
      "[\"It was a dark and stormy night\", \"The old mansion loomed before us\", \"As we stepped inside\", \"The creaking of the floorboards beneath our feet\", \"Suddenly\", \"A loud crash echoed through the hallway\", \"We froze\", \"And then we saw it...\"]\n",
      "\n",
      "I hope you enjoy reading this horror story! Let me know if you have any questions.\n",
      "<class 'str'>\n",
      "['The old mansion loomed before us', 'Crickets chirped in the darkness', 'Rachel just returned home', 'Her eyes scanned the living room', 'But something felt off', 'She noticed the flickering candles', 'And the eerie silence']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "template = \"\"\"Create a story using this genre suggestion: {suggestion}\n",
    "Return the story in the format of a python list data type in chronological order of the story\n",
    "For example [\"It was dark outside\", \"Rachel just returned homm\",\"..\"]\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['suggestion'], template=template)\n",
    "\n",
    "llm = load_model(\"cuda\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
    "\n",
    "suggestion = 'Horror'\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "output = llm_chain.run(suggestion)\n",
    "converted_list = string_to_list(output)\n",
    "print(output)\n",
    "print(type(output))\n",
    "\n",
    "print(converted_list)\n",
    "print(type(converted_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diff",
   "language": "python",
   "name": "stable_diff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
